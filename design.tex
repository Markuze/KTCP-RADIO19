%\subsection{Rate-Control Interface: \oursys Design} 
\section{\oursys Design}\label{sec:design}
\oursys is a Linux kernel-based module, incorporating the four optimizations described in section ~\ref{sec:approx}. In this section we discuss the various design and implementation details. 
%In addition, \oursys enables utilizing easily-deployable commodity VMs, as well as  standard programming APIs (POSIX/\sockets). %Specifically, it relies on 
%commodity VMs in a public cloud environment provide a powerful and flexible platform 
%for our goals, 
%\sockets are time-tested and ubiquitous and thus a clear choice.
%\oursys \cite{ktcp} is a kernel module based on Ubunutu 17.04. The goal of \oursys is to provide an efficient, delay-free TCP optimization, while utilizing commodity VMs and standard programming APIs. Easily-deployable commodity VMs in a public cloud environment provide a powerful and flexible platform for our goals, while \sockets are time-tested and ubiquitous and thus a clear choice.\MA{An overview of the section is needed}

\T{Kernel mode.} We implemented \oursys as a kernel module. We rely on procfs~\cite{proc} to control the behaviour of \oursys. A virtual file system~\cite{virtfs} provides a simple interface and facilitates easy scripting that allows communication with the module at run time.

The decision to use kernel mode is a significant one. While developing in user space would have provided an easier development environment, implementing \oursys in the kernel allows us to (1) take advantage of resources only available in the kernel, such as \textit{Netfilter}~\cite{netfilter}, which is crucial to our needs; and (2) avoid the penalties that stem from numerous \textit{system calls}~\cite{Copy, FlexSC}. By working in the kernel, we eliminate the redundant transitions to and from user space by avoiding gratuitous system calls. Netfilter, provides hooks for callbacks on the network stack is the standard option for capturing and processing a packet in various stages of its path trough the network stack.  

%\T{Design Considerations} 
% While many optimizations we propose in \oursys can be implemented in user space, and therefore provide an easier development environment, some resources are only accessible in the kernel. One downside to using user space is that utilities such as Netfilter~\cite{netfilter}, which is crucial to our needs, are only accessible via the kernel. 
% . 
%Additionally, numerous system calls can hinder performance considerably~\cite{Copy, FlexSC}. 

The decision to implement the components of \oursys in the kernel is further made easy by the fact that all socket APIs have kernel counterparts \footnote{One limitation of an in-kernel implementation is that epoll~\cite{epoll}, a scalable I/O event notification mechanism, has no kernel counterpart}. Instead of POSIX threads we utilize kernel\_threads to handle each leg of the split connections. 
%We have measured the cost of context switching between kernel\_threads on our setup, assuming an n1-standard(GCP) VM with Intel Skylake Xeon CPU. %We measured the time it takes two kernel\_threads to call schedule() 10 million times each. This experiment completes in under 3.2 seconds, resulting in 0.16 \usec per context switch on average; by comparison, an analogous experiment with two processes runs for 15.6 seconds and 12.9 seconds for two POSIX threads~\cite{pthreads}. The experiment highlights the cost %The reason behind the exact difference in performance between user space and  kernel mode is still an open question. In any case, Of course, these are not exactly comparable because the user-space experiments invoke system calls in order to switch context (sched\_yield()). In any case, since we want to avoid system calls and minimize context switches, implementing \oursys in the kernel is the logical choice.

\T{Basic implementation.} The basic implementation of \oursys has three components. (1) A socket listening for incoming connections. (2) Iptable~\cite{iptables} rules that redirect TCP connections to our proxy socket. (3) A second TCP socket is used to connect to the destination and thus complete the second leg of the split connection. Once both connections are established, the bytes of a single stream are read from one socket, and then forwarded to its peer. This forwarding happens in both directions. When either connection is terminated via an error or FIN, the other connection is shut down as well. This means that the bytes in flight (\ie not yet acked) will reach their destination, but no new bytes can be sent.

\T{Buffer size.} We found that the size of the buffer used to read and write the data is important. At first we used  a 4KB buffer, and experienced degraded performance. Starting at 16KB buffer sizes the performance levels out. Each split connection has two sockets and two kernel\_threads~\cite{kthread}. Each split connection takes 50KB of memory to maintain. With  each kernel\_thread taking 9KB in addition to 16KB for the forwarding in each direction.

\T{Early-SYN.} 
%In early SYN~\cite{ladiwala,siracusano2016miniproxy}, a SYN packet is sent to the next-hop server as soon as the SYN packet arrives. This is done without waiting for the three-way handshake to finish. The standard \sockets API does not facilitate the capture of the first TCP SYN packet.
% Netfilter, which provides hooks for callbacks on the network stack is the natural option for capturing and processing a packet in various stages of its path trough the kernel. 
As there is no standard API that enables the capture of the first SYN packet, we use Linux Netfilter~\cite{netfilter} hooks. We add a hook that captures TCP packets, and then parse the headers for the destination and the SYN flag. With this information \oursys launches a new kernel\_thread. Its impossible to creating a new thread while in the Netfilter callback, which is an atomic context. We use our thread pool to lunch kernel\_threads from atomic contexts. The "new" thread initiates a connection to the intended destination. Capturing the SYN allows the \relay to establish the two sides of a connection concurrently.

\T{Thread pool.} We use blocking send/receive calls with our sockets allowing for a simple implementation; this also means that we need a kernel\_thread per active socket. Unfortunately, the creation of a new kernel\_thread is costly. On our setup, a kernel\_thread creation takes about 12\usec, on average.\footnote{By comparison a fork consumes more than 25\usec , while launching a POSIX pthread consumes around 13\usec.} But an outlier may consume several milliseconds, resulting in a jittery behaviour.

To mitigate this problem and the problem of creating new kernel\_threads from atomic context, we create a pool of reusable threads. Each kernel\_thread in this pool is initially waiting in state TASK\_INTERRUPTIBLE (ready to execute). When the thread is allocated, two things happen: (1) a function to execute is set and (2)  the task is scheduled to run (TASK\_RUNNING). When the function is finished executing, the thread returns to state TASK\_INTERRUPTIBLE and back to the list of pending threads, awaiting to be allocated once more. A pool of pre-allocated kernel threads thus removes the overhead of new kernel\_thread creation. A new kernel\_thread from the waiting pool can start executing immediately and can be launched from any context. We have measured the cost of context switching between kernel\_threads on our setup, assuming an n1-standard(GCP) VM with Intel Skylake Xeon CPU. We measured the time it takes two kernel\_threads to call schedule() 10 million times each. This experiment completes in under 3.2 seconds, resulting in 0.16 \usec per context switch on average. \oursys attempts to keep the number of threads in a pool between two configurable watermarks. \oursys creates new threads when the number drops below the low mark and frees from the pool when the limit grows above the high mark. On a multi-core system, the heavy lifting of thread creation is offloaded to a dedicated core.

\T{\reconn.} 
For \reconn, we have added a dedicated server socket that accepts connections from other \relays that may initiate new \reconn \footnote{In order to keep the connection from closing before being used, the sockets are configured with KEEP\_ALIVE}.

When established, these connections await for the destination address to be sent from the initiating peer. The destination address is sent over the connection itself. This information is sent in the very first bytes, and all following bytes belong to the forwarded stream. 
Once the destination address is received, a connection to the destination is established and the second leg of the split connection is established.\\ 
For example a SYN captured on \rc will trigger a look-up in the pool of \reconn on \rc with the address of \rs. If a \reconn exists it will be used for the split connection. \rc will send the target information inside the connection. Upon receiving the target information from \rc, \rs will establish a TCP connection with the target. \rs will forward the stream from \rc to the target. When the connection terminates the socket associated with the connection will be freed and will not be reused.\\ 
 On \reconn sockets Nagle's Algorithm~\cite{nagle} is disabled. In our experiments, we have seen that the TTFB is increased by some $200$ milliseconds, unless Nagle's Algorithm is disabled.  

\T{Proc.} The water marks of the thread-pool, the destination and number or \reconn is controlled via the procfs~\cite{proc} interface.

%\T{Effort.} The total implementation of \oursys is less than 2000 LOC (lines of code). The basic implementation is about 500 LOC, thread pool and early syn add 300 LOC each, and \reconn add 500 LOC. The code for the proc interface and common utilities consists of about 300 LOC.

%\T{Implementation at scale.} We now want to briefly discuss how the existing implementation may be scaled in the future. With 10K split connections, the memory footprint of socket buffers alone; far exceeds the size of the shared L3 cache of most modern servers\footnote{On GCP, it is an impressive 56MB.}. It may be prudent to expand the epoll API to the kernel and thus save the 18KB of memory per split connection. Extending epoll will not be enough, other avenues should be considered as well. One such idea is the socket API; socket API in the kernel is not zero copy. The needless copy can become detrimental ~\cite{Copy} due to an increase in costly memory traffic. Another point to consider is that, network I/O is serviced by interrupts. For a virtual machine, this means expensive VM exits~\cite{Eli, Elvis}. It is well documented that para-virtual devices like~\cite{virtio,vmxnet3} have sub-optimal performance~\cite{Eli, Elvis}. An SRIOV device and a machine with a CPU that supports Intel's vt-d posted interrupts~\cite{posted} may be needed to achieve near bare-metal performance.