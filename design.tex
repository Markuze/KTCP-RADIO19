%\subsection{Rate-Control Interface: \oursys Design} 
\section{Design}
\label{sec:design}
\T{\oursys.} We have developed a novel kernel module called \textit{\oursys} in Ubuntu 17.04, and have made it openly available on Github~\cite{ktcp}. \oursys implements a kernel-based TCP split together with the four improvements of OCD \oursys over the OCD Baseline (\S\ref{sec:rate-control}). 
%The goal of \oursys is to provide an efficient, delay-free TCP optimization, while 
In addition, \oursys enables utilizing easily-deployable commodity VMs, as well as  standard programming APIs (POSIX/\sockets). %Specifically, it relies on 
%commodity VMs in a public cloud environment provide a powerful and flexible platform 
%for our goals, 
%\sockets are time-tested and ubiquitous and thus a clear choice.
%\oursys \cite{ktcp} is a kernel module based on Ubunutu 17.04. The goal of \oursys is to provide an efficient, delay-free TCP optimization, while utilizing commodity VMs and standard programming APIs. Easily-deployable commodity VMs in a public cloud environment provide a powerful and flexible platform for our goals, while \sockets are time-tested and ubiquitous and thus a clear choice.\MA{An overview of the section is needed}

\T{Kernel mode.} We implemented \oursys in kernel mode. We rely on procfs~\cite{proc} to control the behaviour of \oursys. A virtual file system~\cite{virtfs} provides a simple interface and facilitates easy scripting; additionally, it allows to communicate in run time with \oursys.

The decision to use kernel mode is a significant one. While developing in user space would have provided an easier development environment, implementing \oursys in the kernel allows us to (1) take advantage of resources only available in the kernel, such as \textit{Netfilter}~\cite{netfilter}, which is crucial to our needs; and (2) avoid the penalties that stem from numerous \textit{system calls}~\cite{Copy, FlexSC}. By working in the kernel, we eliminate the redundant transitions to and from user space and avoid gratuitous system calls. 


%\T{Design Considerations} 
% While many optimizations we propose in \oursys can be implemented in user space, and therefore provide an easier development environment, some resources are only accessible in the kernel. One downside to using user space is that utilities such as Netfilter~\cite{netfilter}, which is crucial to our needs, are only accessible via the kernel. 
% Netfilter, which provides hooks for callbacks on the network stack is the natural option for capturing and processing a packet in various stages of its path trough the kernel. 
%Additionally, numerous system calls can hinder performance considerably~\cite{Copy, FlexSC}. 

The decision to implement the components of \oursys in the kernel is further made easy by the fact that all socket APIs have kernel counterparts. One limitation of an in-kernel implementation is that epoll~\cite{epoll}, a scalable I/O event notification mechanism, has no kernel API. Instead, we use kernel\_threads to service our sockets. 
We have measured the cost of context switching between kernel\_threads on our setup, assuming an n1-standard(GCP) VM with Intel Skylake Xeon CPU. We measured the time it takes two kernel\_threads to call schedule() 10 million times each. This experiment completes in under 3.2 seconds, resulting in 0.16 \usec per context switch on average; by comparison, an analogous experiment with two processes runs for 15.6 seconds and 12.9 seconds for two POSIX threads~\cite{pthreads}. %The reason behind the exact difference in performance between user space and  kernel mode is still an open question. In any case, 
Of course, these are not exactly comparable because the user-space experiments invoke system calls in order to switch context (sched\_yield()). In any case, since we want to avoid system calls and minimize context switches, implementing \oursys in the kernel is the logical choice.

\T{Basic implementation.} The basic implementation of \oursys relies on three components. (1) We create a socket that listens for incoming connections. (2) Iptable~\cite{iptables} rules redirect specific TCP packets to our proxy socket. (3) A second TCP socket is used to connect to the destination and thus complete the second leg of the split connection. Once both connections are established, the bytes of a single stream are read from one socket, and then forwarded to its peer. This forwarding happens in both directions. When either connection is terminated via an error or FIN, the other connection is shut down as well. This means that the bytes in flight (\ie not yet acked) will reach their destination, but no new bytes can be sent.

\T{Buffer size.} We found that the size of the buffer used to read and write the data is important. At first we used  a 4KB buffer, and experienced degraded performance. However, 16KB and 64KB maintain the same stable speed.

\T{Implementing Early-SYN.} 
%In early SYN~\cite{ladiwala,siracusano2016miniproxy}, a SYN packet is sent to the next-hop server as soon as the SYN packet arrives. This is done without waiting for the three-way handshake to finish. The standard \sockets API does not facilitate the capture of the first TCP SYN packet.
% Netfilter, which provides hooks for callbacks on the network stack is the natural option for capturing and processing a packet in various stages of its path trough the kernel. 
As there is no standard API that enables the capture of the first SYN packet, we use Linux Netfilter~\cite{netfilter} hooks. We add a hook that captures TCP packets, and then parse the headers for the destination and the SYN flag. With this information \oursys launches a new kernel\_thread\footnote{Creating a new thread while in the Netfilter callback is not allowed. We use our thread pool to lunch kernel\_threads from atomic contexts.} that initiates a connection to the intended destination. Capturing the SYN allows the \proxies to establish the two sides of a connection concurrently.

\T{Implementing the thread pool.} 
Each split connection is handled by two dedicated kernel\_threads~\cite{kthread}. Each thread receives from one socket and writes to its peer. One thread is responsible for one direction of the connection. We use blocking send/receive calls with our sockets allowing for a simple implementation; this also means that we need a kernel\_thread per active socket. Unfortunately, the creation of a new kernel\_thread is costly. On our setup, a kernel\_thread creation takes about 12\usec, on average.\footnote{By comparison a fork consumes more than 25\usec , while launching a POSIX pthread consumes around 13\usec.} But an outlier may consume several milliseconds, resulting in a jittery behaviour.

To mitigate this problem and the problem of creating new kernel\_threads from atomic context, we create a pool of reusable threads. Each kernel\_thread in this pool is initially waiting in state TASK\_INTERRUPTIBLE (ready to execute). When the thread is allocated, two things happen: (1) a function to execute is set and (2)  the task is scheduled to run (TASK\_RUNNING). When the function is finished executing, the thread returns to state TASK\_INTERRUPTIBLE and back to the list of pending threads, awaiting to be allocated once more. A pool of pre-allocated kernel threads thus removes the overhead of new kernel\_thread creation. A new kernel\_thread from the waiting pool can start executing immediately and can be launched from any context. When the pool is exhausted, \ie all pool threads are running, a new thread will be allocated; thus for best performance the pool-size should be configured to cover the maximum possible number of concurrent connections. On a multi-core system, the heavy lifting of thread creation is offloaded to a dedicated core. This core executes a thread that allocates new kernel\_threads any time the pool size dips under some configurable value. On the other hand, when threads return to the pool, it is possible to conserve system resources by restricting the number of threads awaiting in the pool and freeing the execs threads.\footnote{Each kernel\_thread consumes 9KB of memory.}

% The creation of a new kernel\_thread for each new TCP connection between the two cloud relays is costly. On our setup it takes XXX milliseconds\MA{I plan to run a small experiment to measure the time it takes to create a kernel\_thread, a fork and a POSIX pthread}. This delay is added to the connection creation time \IK{well, also to the download time, etc.; maybe just delete this sentence?}. To mitigate this problem, we create a pool of reusable kernel threads, which are empty threads waiting to be assigned to new connections. This eliminates the overhead of new kernel\_thread creation.

\T{Implementing the \reconn.} 
To implement \reconn, we have added a second server socket. Unlike the ``proxy'' socket, this socket listens for connections from other \proxies that are initiating new \reconn. In order to keep the connection from closing before it is allocated, the sockets are configured with KEEP\_ALIVE.

When established, these connections await for the destination address to be sent from the initiating peer. The destination address is sent over the connection itself. This information is sent in the very first bytes, and all following bytes belong to the forwarded stream. 
Once the destination address is received, a connection to the destination is established and the second leg of the split connection is established. The streams are forwarded between the sockets just like in the basic design.

We found that Nagle's Algorithm~\cite{nagle} should be disabled on these sockets. In our experiments, we have seen that without disabling it, the time-to-first-byte is increased by some $200$ milliseconds.  

\T{Proc.} The size of the thread-pool, the destination of a \reconn, and their number are controlled via the procfs~\cite{proc} interface.

\T{Effort.} The total implementation of \oursys is less than 2000 LOC (lines of code). The basic implementation is about 500 LOC, thread pool and early syn add 300 LOC each, and \reconn add 500 LOC. The code for the proc interface and common utilities consists of about 300 LOC.

\T{Implementation at scale.} We now want to briefly discuss how the existing implementation may be scaled in the future. With 10K split connections, the memory footprint of socket buffers alone; far exceeds the size of the shared L3 cache of most modern servers\footnote{On GCP, it is an impressive 56MB.}. It may be prudent to expand the epoll API to the kernel and thus save the 18KB of memory per split connection. Extending epoll will not be enough, other avenues should be considered as well.
One such idea is the socket API; socket API in the kernel is not zero copy. The needless copy can become detrimental ~\cite{Copy} due to an increase in costly memory traffic. Another point to consider is that, network I/O is serviced by interrupts. For a virtual machine, this means expensive VM exits~\cite{Eli, Elvis}. It is well documented that para-virtual devices like~\cite{virtio,vmxnet3} have sub-optimal performance~\cite{Eli, Elvis}. An SRIOV device and a machine with a CPU that supports Intel's vt-d posted interrupts~\cite{posted} may be needed to achieve near bare-metal performance.